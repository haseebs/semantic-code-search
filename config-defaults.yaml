seed:
  value: 0
  desc: 
model_type:
  desc:
  value: transformer_model
code_encoder_type:
  desc:
  value: "self_attention_encoder"
query_encoder_type:
  desc:
  value: "self_attention_encoder"
encoder_sharing_mode:
  desc:
  value: "per_input_source"
code_max_num_tokens:
  desc:
  value: 200
query_max_num_tokens:
  desc:
  value: 30
learning_rate:
  desc:
  value: 0.001
batch_size:
  desc:
  value: 128
loss:
  desc:
  value: "cosine"
vocab_size:
  desc:
  value: 10000
embedding_dim:
  desc:
  value: 128
dropout_prob:
  desc:
  value: 0.1
gradient_clip:
  desc:
  value: 1
margin:
  desc:
  value: 0.1
max_epochs:
  desc:
  value: 300
patience:
  desc:
  value: 5
use_bpe:
  desc:
  value: True
vocab_pct_bpe:
  desc:
  value: 0.5
vocab_count_threshold:
  desc:
  value: 10
keep_keys:
  desc:
  value: ["language", "docstring_tokens", "code_tokens", "code_ast_tokens", "code_ast_descendants"]
keep_keys_test:
  desc:
  value: ["language", "docstring_tokens", "code_tokens", "docstring", "code", "code_ast_tokens", "code_ast_descendants"]
data_dirs:
  desc:
  value: ["../data/data_ast/ruby/final/jsonl"]
self_attention_nheads:
  desc:
  value: 8
self_attention_nhid:
  desc:
  value: 512
self_attention_nlayers:
  desc:
  value: 3
clamping_distance:
  desc:
  value: 3
tree_transformer_use_positional_embeddings:
  desc: "Use sinusoidal positional embeddings a long with relative MHA"
  value: False